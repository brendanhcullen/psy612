---
title: "Lab 3: Univariate Regression (II) & GLM"
output: 
  html_document:
    include:
      in_header: header.html 
    fig_caption: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
    toc_float: TRUE
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, rows.print = 10)

# suppress scientific notation
options(scipen = 999)
```

```{r lab-3-1, echo=FALSE, message=FALSE}
library(tidyverse) # for plotting and data wrangling
library(rio) # for importing data
library(broom) # for cleaning up output
library(sjPlot) # for plotting
library(ggpubr) # for plotting
library(carData) # for Guyer dataset
```


# Purpose

* Today we will briefly review univariate regression and then will discuss how to summarize and visualize uncertainty in regression models using a variety of plotting methods. We will then touch on how to estimate regression coefficients using matrix algebra. Lastly, we will introduce the General Linear Model and demonstrate how GLM can be used to understand all of the statistical tests we have learned so far (*t*-tests, ANOVA, correlations, regressions) within one (beautiful!) unifying framework.  

* We're going to work with a dataset about the relationship between personality and health. Let's pretende weâ€™ve collected data from 60 people (30 men and 30 women) on their self-reported conscientiousness (using the [Big Five Inventory](https://www.ocf.berkeley.edu/~johnlab/bfi.htm){target="_blank"}) and their self-rated physical health.

```{r lab-3-2, echo=FALSE}
health <- import("https://raw.githubusercontent.com/uopsych/psy612/master/labs/lab-3/data/consc_health.csv")
```

*To quickly navigate to the desired section, click one of the following links:*

1. [Visualizing uncertainty in regression](#uncertainty)
1. [Regression with matrix algebra](#matrix)
1. [The General Linear Model](#glm)

***

# Visualizing uncertainty in regression{#uncertainty}

* Before we discuss how to visualize uncertainty in regression, let's quickly review how to estimate a regression model in R. 

<br>

* Conducting regressions in R is actually pretty simple. We use the `lm()` function which is part of the pre-loaded {stats} library. There are basically two ingredients we pass to the `lm()` function

1. **The formula:** Specify your regression formula in the general form `y ~ x`.

2. **The data:** the dataframe that contains the variables in the formula. This is technically optional.

<br>

* Recall how we wrote out our model, specifying self-rated health as a function of conscientiousness

$$Y_i = b_0 + b_1X_i + e_i$$

$$sr\_health_i = b_0 + b_1consc_i + e_i$$
<br>

* Fill in the blanks below to specify the model

<!---LEARNR EX 1-->

<iframe style="margin:0 auto; min-width: 100%;" id="lab-3-glm-ex1" class="interactive" src="https://brendancullen.shinyapps.io/lab-3-glm-ex1/" scrolling="no" frameborder="no"></iframe>

<!------------->

```{r echo=FALSE, results='hide'}
model <- lm(sr_health ~ consc, data = health)
summary(model)
```


* Here are our coefficients...

```{r lab-3-4, echo=FALSE}
tidy(model) %>% #
  rename(coefficient = term,
        b = estimate,
        SE = std.error,
        t = statistic,
        p = p.value) %>%
  mutate(p = ifelse(p > .001, round(p, 3), "< .001")) %>% 
  knitr::kable(digits = c(NA, 2, 2, 2, 3), 
               caption = "Results of Regressing Self-Reported Health on Conscientiousness") 
```
<br>

> **Question:** What do the intercept and slope mean? What do the *t*-values tell us? 


## Confidence intervals

* Our `b's` (intercept and slope) are *estimates* from our sample of true population parameters ($\beta$'s). Remember that whenever we calculate an estimate of something, we should also determine how precise our estimate is. This is where standard errors and confidence intervals come in. 

* Recall the formula for calculating confidence intervals:

$$CI_b = b \pm critical\_value * SE_b$$

* In [Minihack 2](#minihack2) you will get some practice using this formula to calculate confidence intervals around regression coefficients. For now, we will use a much easier method: `stats::confint()`. This function takes in a fitted model object as the first argument. By default it will give you 95% CI's. 

####  {.tabset .tabset-fade .tabset-pills}

##### Code

```{r lab-3-5, eval=FALSE}
confint(model)
```

##### Output

```{r lab-3-6, echo=FALSE, ref.label='lab-3-5'}
```

####
<br>

>**Question:** What does these 95% CI for the slope of conscientiousness mean in plain English? 


## Confidence bands

* In addition to estimating precision around the our coefficients, we can also estimate our precision around each predicted value, $\hat{Y_i}$. These standard errors are generated by `broom::augment()` (and are labeled `.se.fit`).

####  {.tabset .tabset-fade .tabset-pills}

##### Code

```{r lab-3-7, eval=FALSE}
model %>% # start with our model object
  augment() %>% # from broom package; gives us fitted values, residuals, etc.
  select(sr_health, .fitted, .se.fit) # select relevant variables
```

##### Output

```{r lab-3-8, echo=FALSE, ref.label='lab-3-7'}
```

####
<br>

* If we were to string all of this information together, it would generate a confidence **band** around our regression line. It's really easy to get this confidence band when creating a scatter plot by adding `geom_smooth(method = "lm")`. 

<br> 

* Fill in the blanks in the code below to generate a scatter plot with a regression line and 95% confidence band. 

<!---LEARNR EX 2-->

<iframe style="margin:0 auto; min-width: 100%;" id="lab-3-glm-ex2" class="interactive" src="https://brendancullen.shinyapps.io/lab-3-glm-ex2/" scrolling="no" frameborder="no"></iframe>

<!------------->





<script>
  iFrameResize({}, ".interactive");
</script>
